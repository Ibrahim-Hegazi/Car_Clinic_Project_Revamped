# ğŸ§  Car Clinic Smart Repair Advisor ğŸš—ğŸ› ï¸

A fully autonomous, LLM-powered data pipeline that extracts real-world automotive problems from Reddit, cleans, structures, augments, and tags them, then intelligently recommends the most appropriate Car Clinic repair branch using semantic similarity, tags, and location. The system culminates in a real-time LLM-powered Emergency Repair Assistant for customers and mechanics.

---

## ğŸ“Œ Table of Contents

- [ğŸ” Project Overview](#-project-overview)
- [ğŸš€ Final Goals](#-final-goals)
- [ğŸ Competitors](#-competitors)
- [â— Challenges Faced](#-challenges-faced)
- [â›” Project Roadblocks](#-project-roadblocks)
- [ğŸ’¡ Suggested Solutions](#-suggested-solutions)
- [ğŸ“ˆ System Architecture](#-system-architecture)
- [ğŸ› ï¸ Features](#-features)
- [ğŸ§ª Pipeline Phases](#-pipeline-phases)
- [ğŸ§¬ Data Flow Diagram](#-data-flow-diagram)
- [ğŸ—‚ Directory Structure](#-directory-structure)
- [âš™ï¸ Tech Stack](#-tech-stack)
- [âš¡ Getting Started](#-getting-started)
- [ğŸ§  Prompt Engineering Principles](#-prompt-engineering-principles)
- [ğŸ—“ Roadmap](#-roadmap)
- [ğŸ§¾ License](#-license)
- [ğŸ‘¨â€ğŸ’» Author](#-author)
- [ğŸ“¬ Future Improvements](#-future-improvements)
- [ğŸ™‹â€â™‚ï¸ Contributing](#-contributing)
- [ğŸ“ Contact](#-contact)

---

## ğŸ” Project Overview

**Car Clinic Smart Repair Advisor** is an intelligent, modular system that reads thousands of Reddit threads from car repair subreddits, cleans and structures the data using LLMs, augments and tags content, and then recommends the best-fit repair branch using semantic similarity, embeddings, and geographic filters.

This enables:  
- âš™ï¸ Real-time, explainable repair suggestions  
- ğŸ¤– LLM inference  
- ğŸŒ Multilingual data augmentation and understanding  
- ğŸ§­ Nearest optimal repair branch recommendations  
- ğŸ’¬ An interactive chatbot interface for customers and mechanics  

---

## ğŸš€ Final Goals

- âœ… Autonomous pipeline: From daily Reddit scraping to real-time recommendations.  
- âœ… LLM processing: Clean noisy car repair data into structured problemâ€“solution pairs.  
- âœ… Semantic tagging and embeddings: Enrich issue understanding and enable vector similarity.  
- âœ… Smart Branch Recommender: Match user problems with the best nearby branch based on tags, embeddings, and availability.  
- âœ… Emergency LLM Chatbot: Provide instant fixes and guidance to mechanics and users in real-time.  
- âœ… API + CI/CD Ready: Modular FastAPI backend with GitHub Actions and Prefect orchestration.  
- âœ… Fully documented: Complete with data samples, diagrams, testing artifacts, and prompt design logic.

---

## ğŸ Competitors

Several projects and platforms tackle automotive problem diagnosis and repair recommendations using AI and data-driven approaches. Notable competitors include:

- **[RepairPal](https://repairpal.com/)**: Offers cost estimates and nearby repair shops but lacks real-time AI-based issue parsing from community data.  
- **[YourMechanic](https://why.yourmechanic.com/)**: Provides on-demand mechanic services and diagnostics but doesn't leverage large-scale social data for problem insights.  
- **[CarMD](https://carmd.com/)**: Focuses on OBD-II diagnostic tools rather than community-driven repair advice.  
- **Open-source automotive chatbot projects**: Most lack integration with live community data sources (e.g., Reddit) and LLM Data Cleaning. Example Project: [car_maintenance_chatbot_project](https://github.com/zebmuhammad/car_maintenance_chatbot_project/tree/main)

---

## â— Challenges Faced

- **Data Noise and Quality**: Reddit data contains spam, bot posts, slang, and irrelevant content making cleaning complex.  
- **LLM Offline Processing**: Running large language models locally for data cleaning and understanding requires significant compute and optimization. Therefore leading to other solutions that might be costly.
- **Ollama-Based LLM Processing Time**: Current data cleaning with Ollama-based LLMs, while providing high-quality structured outputs, is time-intensive. For example, processing around 700 rows in Phase 2 can take approximately 31 hours to complete. This runtime presents a bottleneck for scaling and requires optimization or infrastructure improvements to achieve timely data processing.
- **Tagging Consistency**: Creating a comprehensive yet manageable tag schema for diverse car issues and mechanic specialties is difficult.  
- **Semantic Matching Accuracy**: Aligning user problems with correct branches involves fine-tuning embeddings and filter heuristics.  
- **Multilingual and Slang Variations**: Handling multiple languages and informal expressions adds complexity to augmentation and translation.  
- **Integration Complexity**: Combining multiple phasesâ€”data extraction, cleaning, tagging, embedding, recommendation, and chatbotâ€”requires robust orchestration.

---

## â›” Project Roadblocks

- Limited computational resources for efficient offline LLM inference slowed data cleaning throughput.  
- Incomplete or evolving tag schemas caused misclassification in early testing phases.  
- Integration of real-time chatbot with backend recommender still in prototype stage, delaying deployment.  
- Dataset imbalance due to sparse comments or rare issues affected model generalization.  
- Ongoing challenges in automating end-to-end orchestration with retries and failure handling.

---


## ğŸ’¡ Suggested Solutions

- **Optimize LLM Models**: Explore quantized models or smaller LLMs with comparable performance to speed up offline inference.
- **Refine Tagging Ontology**: Collaborate with domain experts to finalize tag schema and implement automated tag validation.  
- **Enhance Data Pipeline**: Implement advanced filters and anomaly detection to reduce noise upstream.  
- **Improve Chatbot Integration**: Develop and test API endpoints for seamless real-time interactions and expand frontend support.  
- **Expand Multilingual Support**: Integrate additional language models and fine-tune back-translation workflows.  
- **Robust Orchestration**: Extend Prefect flows with better logging, alerting, and retries to improve pipeline stability.  
- **User Feedback Loop**: Design mechanisms to collect user feedback for continuous model improvements and retraining.  
- **Cloud Deployment Planning**: Prepare for scalable deployment using containerization and managed cloud services.  

---

## ğŸ§ª Pipeline Phases

<details>
<summary>âœ… Phase 1: Reddit Data Extraction (Scraping)</summary>

- ğŸ” **Inputs:**
  - List of subreddits
  - Reddit API credentials (via `praw`)
  - Configs (e.g., number of posts, filters)

- âš™ï¸ **Inside:**
  - Fetch top daily/weekly posts with comments
  - Remove posts with no comments
  - Filter spam/bot content
  - Save results in `/data/raw/` as JSON or CSV

- ğŸ¯ **Purpose:**  
  Collect relevant raw text data (real-world issues and discussions) for downstream LLM processing.

- ğŸ” **Used again in:**  
  Phase 2 (Cleaning), Phase 10 (Re-training or evaluation for LLMs)

- ğŸ“¤ **Outputs:**  
  `/data/raw/reddit_posts_with_comments.json`

</details>

<details>
<summary>âœ… Phase 2: Reddit Data Cleaning (LLM-Based)</summary>

- ğŸ” **Inputs:**  
  - Raw Reddit posts + top comments  
  - LLM model (offline or Ollama)  
  - Prompt template

- âš™ï¸ **Inside:**  
  - Preprocessing: Remove bots, normalize text  
  - LLM Inference: Extract (problem â†’ solution) pairs using prompts  
  - Postprocessing: JSON formatting, hallucination checks, null handling

- ğŸ¯ **Purpose:**  
  Converts noisy internet content into clean problemâ€“solution pairs for chatbot and tagging.

- ğŸ” **Used again in:**  
  Phase 3 (Tag Generation), Phase 9 (LLM chatbot fine-tuning)

- ğŸ“¤ **Outputs:**  
  `/data/cleaned/cleaned_problems_solutions.json`

</details>

<details>
<summary>ğŸ¦‘ Phase 3: Data Augmentation & Translation</summary>

- ğŸ” **Inputs:**  
  - Cleaned problemâ€“solution pairs  
  - NLPAug/TextAttack or offline LLMs for paraphrasing  
  - Optional translation APIs or offline models  
  - Noise injection rules (typos, slang)

- âš™ï¸ **Inside:**  
  - Paraphrasing: Generate 1â€“3 semantically similar versions  
  - Translation: Translate â†’ Back-translate (e.g., EN â†’ AR â†’ EN)  
  - Noise Injection: Add typos, abbreviations  
  - Flow management: `augmenter/flow.py`, `translator/flow.py`

- ğŸ¯ **Purpose:**  
  Increase data diversity and robustness to phrasing variability and multilingual input.

- ğŸ” **Used again in:**  
  Phase 5 (Embedding generation), Phase 10 (Chatbot understanding)

- ğŸ“¤ **Outputs:**  
  `/data/augmented/augmented_problems_solutions.json`

</details>

<details>
<summary>ğŸŒ¿ Phase 4: Tag Generator (Problem + Solution Tags)</summary>

- ğŸ” **Inputs:**  
  - Cleaned or augmented problemâ€“solution pairs  
  - Tagging rules or LLM model  
  - Optional keyword dictionaries or tag schemas

- âš™ï¸ **Inside:**  
  - Extract semantic tags from problems and solutions  
  - Track source (rule-based, LLM, or hybrid)  
  - Store metadata like confidence, LLM version  
  - Orchestrated via `tag_generator/flow.py`

- ğŸ¯ **Purpose:**  
  Enables structured understanding for tag-based filtering and scoring in recommendations.

- ğŸ” **Used again in:**  
  Phase 6 (Tag-based matching), Phase 10 (Chatbot explanations)

- ğŸ“¤ **Outputs:**  
  `/data/tagged/tagged_problems_solutions.json`

</details>

<details>
<summary>ğŸ”¢ Phase 5: Embedding Generation (Problems + Branches)</summary>

- ğŸ” **Inputs:**  
  - Cleaned/tagged problemâ€“solution pairs  
  - Branch expertise descriptions  
  - Pretrained embedding model (e.g., Sentence-BERT, Instructor-XL)

- âš™ï¸ **Inside:**  
  - Vectorize problemâ€“solution pairs  
  - Vectorize branch expertise profiles  
  - Store embeddings separately (`/data/embeddings/problems/`, `/data/embeddings/branches/`)  
  - Auto-skip already embedded entries  
  - Freeze model versions & store hashes  
  - Flow handled by `embedding_generator/flow.py`

- ğŸ¯ **Purpose:**  
  Enables similarity-based retrieval and matching for hybrid recommendations.

- ğŸ” **Used again in:**  
  Phase 6 (Similarity scoring), Phase 10 (Chatbot reasoning)

- ğŸ“¤ **Outputs:**  
  `/data/embeddings/problems/*.npy`, `/data/embeddings/branches/*.npy`

</details>

<details>
<summary>ğŸ—ºï¸ Phase 6: Branch Recommender System</summary>

- ğŸ” **Inputs:**  
  - Tagged problems  
  - Problem embeddings  
  - Branch embeddings + tag profiles  
  - Branch availability + location (optional)

- âš™ï¸ **Inside:**  
  - Match tags (e.g., Jaccard Index)  
  - Match vectors (cosine similarity)  
  - Apply location filter if coordinates provided  
  - Composite scoring: weighted formula of tags, embeddings, location  
  - Return top-N recommendations with explainability logs  
  - Flow: `branch_recommender/flow.py`

- ğŸ¯ **Purpose:**  
  Core logic to choose the best-fit repair branch per user query.

- ğŸ” **Used again in:**  
  Phase 10 (Chatbot resolution), Phase 11 (Backend endpoint)

- ğŸ“¤ **Outputs:**  
  `/data/recommendations/top_branches_for_postid.json`

</details>

<details>
<summary>ğŸ§ª Phase 7: Local & Integrated Testing</summary>

- ğŸ” **Inputs:**  
  - Outputs from previous phases  
  - Small manually crafted test batch  
  - Expected results/ground truth (if available)

- âš™ï¸ **Inside:**  
  - Run unit tests per script  
  - Run integration tests on test batch  
  - Visualize embeddings, matches, tags  
  - Store snapshots in `/docs/test_cases/`

- ğŸ¯ **Purpose:**  
  Verify correctness and integration before scaling.

- ğŸ” **Used again in:**  
  Phase 12 (Documentation), CI/CD (Phase 9)

- ğŸ“¤ **Outputs:**  
  `/docs/test_cases/*.json`, `/docs/test_results/`, visuals

</details>

<details>
<summary>ğŸŒ€ Phase 8: Prefect Orchestration</summary>

- ğŸ” **Inputs:**  
  - All flow.py scripts (Phases 1â€“6)  
  - Prefect config (retry, logging)  
  - Optional Prefect Cloud credentials

- âš™ï¸ **Inside:**  
  - Convert scripts to Prefect tasks  
  - Chain tasks in logical order  
  - Add retries, error handlers, logging  
  - Trigger from CLI or schedule

- ğŸ¯ **Purpose:**  
  Automate and connect pipeline parts in modular robust system.

- ğŸ” **Used again in:**  
  Phase 9 (CI/CD), Phase 11 (Runtime scheduling)

- ğŸ“¤ **Outputs:**  
  Prefect DAG, CLI runnable flows, logs

</details>

<details>
<summary>â˜ï¸ Phase 9: GitHub Actions & Deployment</summary>

- ğŸ” **Inputs:**  
  - GitHub repo + workflows  
  - Prefect-compatible flows  
  - Secrets (.env or GitHub Secrets)

- âš™ï¸ **Inside:**  
  - Run flows (scraping, cleaning, tagging, embedding, matching)  
  - Scheduled daily (e.g., 12:15 PM Egypt time)  
  - Support matrix builds and parallelization  
  - Optional Docker container builds

- ğŸ¯ **Purpose:**  
  Fully automated data ingestion & processing pipeline on GitHub infrastructure.

- ğŸ” **Used again in:**  
  All phases (1â€“6), redeployment on code updates

- ğŸ“¤ **Outputs:**  
  Daily updated `/data/`, GitHub CI logs, optional Docker images

</details>

<details>
<summary>ğŸ“˜ Phase 10: LLM Chatbot Engine</summary>

- ğŸ” **Inputs:**  
  - User query (via REST API)  
  - Cleaned + tagged Reddit problems  
  - Embeddings (problems & branches)  
  - Branch metadata (tags, location)

- âš™ï¸ **Inside:**  
  - Classify query intent  
  - Retrieve similar Reddit cases  
  - LLM generates structured response  
  - Match to branch via Phase 6 logic  
  - Format JSON response for chatbot

- ğŸ¯ **Purpose:**  
  Frontline AI interaction interface.

- ğŸ” **Used again in:**  
  Phase 11 (API routes), Phase 12 (docs)

- ğŸ“¤ **Outputs:**  
  Structured JSON `{ "solution": ..., "branch": ..., "confidence": ... }`

</details>

<details>
<summary>ğŸšª Phase 11: Backend Integration (FastAPI)</summary>

- ğŸ” **Inputs:**  
  - Chatbot logic (Phase 10)  
  - Recommender logic (Phase 6)  
  - Processed data (embeddings, tags)  
  - API config & schema

- âš™ï¸ **Inside:**  
  - REST endpoints: `/chat/solve`, `/recommend/branch`  
  - Parse inputs, run logic, return JSON  
  - Dockerized for modular deployment  
  - Optional Redis caching

- ğŸ¯ **Purpose:**  
  Expose system via API for production apps.

- ğŸ” **Used again in:**  
  Real-time deployment, frontend integration

- ğŸ“¤ **Outputs:**  
  `main.py` FastAPI server, OpenAPI docs

</details>

<details>
<summary>ğŸ“˜ Phase 12: Documentation & Finalization</summary>

- ğŸ” **Inputs:**  
  - All code, flows, configs, data samples  
  - Testing results (Phase 7)  
  - Model & LLM choices

- âš™ï¸ **Inside:**  
  - Create README, architecture diagrams  
  - Document phases and modules  
  - Glossary, schema definitions  
  - Data samples & test outputs

- ğŸ¯ **Purpose:**  
  Make pipeline shareable, reproducible, production-ready.

- ğŸ” **Used again in:**  
  Onboarding, public release, presentations

- ğŸ“¤ **Outputs:**  
  `/docs/`, `README.md`, diagrams, prompt designs, schema

</details>

